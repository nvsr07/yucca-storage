<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Note per la gestione del cluster MongoDB &mdash; CSI MongoDB 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="CSI MongoDB 1.0 documentation" href="index.html" />
    <link rel="next" title="Java driver assessment" href="upgrade_java_driver.html" />
    <link rel="prev" title="Best Practices per Availability e Consistenza" href="availability.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="upgrade_java_driver.html" title="Java driver assessment"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="availability.html" title="Best Practices per Availability e Consistenza"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">CSI MongoDB 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="note-per-la-gestione-del-cluster-mongodb">
<h1>Note per la gestione del cluster MongoDB<a class="headerlink" href="#note-per-la-gestione-del-cluster-mongodb" title="Permalink to this headline">¶</a></h1>
<p>Di seguito sono riportate alcune note e procedure di utilità durante la gestione di un cluster
mongodb.</p>
<div class="section" id="oplog-consumption-estimation">
<h2>OpLog Consumption Estimation<a class="headerlink" href="#oplog-consumption-estimation" title="Permalink to this headline">¶</a></h2>
<p>La <strong>OpLog Window</strong> è la definizione di quanto tempo ci va per fare un giro completo
dell&#8217;oplog. Quindi quanto tempo un nodo del sistema può restare sganciato dal replica set
essendo poi capace di riagganciarsi.</p>
<p>Qualora l&#8217;oplog non sia ancora stato consumato interamente si può calcolare
le informazioni riguardanti il consumo ed il tempo rimanente dello stesso tramite
il seguente script:</p>
<div class="highlight-python"><div class="highlight"><pre>#!/bin/bash
MONGOPATH=&quot;/opt/mongodb/bin/mongo --port 27018&quot;

$MONGOPATH --eval &#39;
    var oplog_speed = db.getReplicationInfo()[&quot;usedMB&quot;] / db.getReplicationInfo()[&quot;timeDiff&quot;];
    var oplog_size = db.getReplicationInfo()[&quot;logSizeMB&quot;];
    var oplog_size_left = oplog_size - db.getReplicationInfo()[&quot;usedMB&quot;];
    var oplog_time = db.getReplicationInfo()[&quot;timeDiffHours&quot;];

    print(&quot;&quot;);
    print(&quot;OpLog Window: &quot;, (oplog_size / oplog_speed / 3600).toFixed(2), &quot;Hours&quot;);
    print(&quot;OpLog Time: &quot;, oplog_time, &quot;Hours&quot;);
    print(&quot;OpLog Size: &quot;, oplog_size.toFixed(2), &quot;MB&quot;);
    print(&quot;OpLog Size Left: &quot;, oplog_size_left.toFixed(2), &quot;MB&quot;);
    print(&quot;OpLog Throughput: &quot;, oplog_speed.toFixed(3), &quot;MB/sec&quot;);
    print(&quot;OpLog TimeLeft: &quot;, (oplog_size_left / oplog_speed / 3600).toFixed(2), &quot;Hours Left&quot;);
&#39;
</pre></div>
</div>
<p>I risultati riportati sono:</p>
<blockquote>
<div><ul class="simple">
<li>OpLog Window -&gt; Quanto tempo è stimato per un giro completo di oplog.</li>
<li>OpLog Time -&gt; Quanto tempo è registrato nell&#8217;oplog
(Se l&#8217;oplog è già consumato questo valore corrisponde alla <strong>OpLog Window</strong>).</li>
<li>OpLog Size -&gt; La dimensione allocata per l&#8217;oplog.</li>
<li>OpLog Size Left -&gt; Quanto dell&#8217;oplog resta ancora disponibile, se l&#8217;oplog è già consumato
questo valore sarà ~0.</li>
<li>OpLog Throughput -&gt; Quale è la velocità con cui è consumato l&#8217;oplog.</li>
<li>OpLog TimeLeft -&gt; Quanto tempo è rimanente nell&#8217;OpLog, se l&#8217;oplog è già consumato
questo valore sarà ~0.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="scaling-orizzontale-vs-scaling-verticale">
<h2>Scaling Orizzontale VS Scaling Verticale<a class="headerlink" href="#scaling-orizzontale-vs-scaling-verticale" title="Permalink to this headline">¶</a></h2>
<p>Al fine di massimizzare l&#8217;efficacia dello scaling verticale è necessario
scaricare il lavoro di lettura sui nodi secondary del replica set.
Come indicato in <a class="reference internal" href="availability.html#read-consistency"><em>Replication Read Consistency</em></a> questo introduce una serie di
problematiche di consistenza che rendono difficile lo sviluppo dell&#8217;applicazione.</p>
<p>Per questa ragione si consiglia di mantenere le letture dei nodi in <tt class="docutils literal"><span class="pre">primary</span></tt>
ed usare le repliche solo a fini di affidabilità.</p>
<p>Qualora la potenza di calcolo del <tt class="docutils literal"><span class="pre">primary</span></tt> non fosse più sufficiente a sottostare
al carico si suggerisce di scalare orizzontalmente con nuovi shard, che permetterebbe
di distribuire il carico tra i nodi senza dover rinunciare alla consistenza.</p>
</div>
<div class="section" id="monitoraggio-cluster">
<h2>Monitoraggio Cluster<a class="headerlink" href="#monitoraggio-cluster" title="Permalink to this headline">¶</a></h2>
<p>Di seguito sono riportati i valori rilevanti monitorati da Nagios,
per ognuno di essi è indicato il significato, quale è il loro valore
all&#8217;interno del monitoraggio e la soglia di attenzione:</p>
<blockquote>
<div><ul>
<li><p class="first"><strong>Load Average</strong>
Se il load average è superiore al numero di CPU disponibili
nella macchina è indice che la CPU non è sufficiente a reggere il carico.</p>
</li>
<li><p class="first"><strong>Disk IO</strong>
Qualora il throuput di io su disco si avvicini a quello monitorato
sul disco usando <tt class="docutils literal"><span class="pre">mongoperf</span></tt> è indice che le performance del disco non sono
sufficienti. Verificare prima se l&#8217;elevato uso del disco non è in verità dovuto
ad un insufficiente quantitativo di RAM.</p>
</li>
<li><p class="first"><strong>Check fs /data</strong>
La directory <tt class="docutils literal"><span class="pre">/data</span></tt> è dove MongoDB solitamente salva i file del database,
è importante monitorarne l&#8217;occupazione in quanto in caso di necessità di riparare
il DB sarà necessario che la sua occupazione non superi il 50%.</p>
</li>
<li><p class="first"><strong>Connect Check</strong>
Riporta se il nodo monitorato riesce a collegarsi al primary del replica set
a cui appartiene in tempi rapidi. E` utile a monitorare la qualità di connessione
verso il primary node del replicat, cosa che influenza la velocità di replicazione.
Qualora il nodo sia il primary stesso non da informazioni utili.</p>
</li>
<li><p class="first"><strong>Page Faults</strong>
Riporta il numero di page fault effettuati durante l&#8217;uso del database, qualora il
numero sia significativo significa che il database non ha sufficiente memoria per
gestire il carico di lavoro.</p>
</li>
<li><p class="first"><strong>Flush Average</strong>
Riporta il tempo richiesto per la scrittura su disco degli inserimenti, se questo valore
è troppo alto può rallentare la scrittura e replicazione dell&#8217;oplog a causa del lock che
viene posto sulle operazione. Se la soglia di errore viene raggiunta può essere necessario
operare per migliorare le prestazioni del disco. O aumentare la RAM per ridurre l&#8217;impatto
sul disco.</p>
</li>
<li><p class="first"><strong>Free Connections</strong>
Monitora il numero di connesisoni usate tra quelle disponibili per i client. Quando il
massimo viene raggiunto non sarà più possibile connettersi per i client.</p>
</li>
<li><p class="first"><strong>Lock Percentage</strong>
Monitora la % che il database spende bloccato sui lock delle operazioni. Questo valore
dovrebbe essere sempre minimizzato in quanto impedisce a qualsiasi operazione di procedere
inclusa la replicazione.</p>
</li>
<li><p class="first"><strong>Memory Usage</strong>
Di questo valore è importante tenere presente i valori <tt class="docutils literal"><span class="pre">mapped</span></tt> che sta ad indicare la
dimensione massima utile perché il DB lavori senza uso del disco e <tt class="docutils literal"><span class="pre">resident</span></tt> che indica
quanta memoria è in uso effettivamente da MongoDB.</p>
</li>
<li><p class="first"><strong>Replicaset Master Monitor</strong>
Verifica se è cambiato il primary node del replica set.</p>
</li>
<li><p class="first"><strong>Replication Lag</strong>
Verifica la latenza di replicazione, quindi quanto i secondary node sono in ritardo rispetto
al nodo primary. Eventuali backups andrebbero sempre effettuati quando i nodi sono in pari.
Se la replicazione è molto in ritardo può essere dovuto alle seguenti cause:</p>
<blockquote>
<div><ul class="simple">
<li>Lentezza di rete tra i nodi</li>
<li>Troppo carico di operazioni sul primary node</li>
<li>Operazioni che detengono il &#8220;lock&#8221; a lungo (verificabile con <tt class="docutils literal"><span class="pre">db.currentOp()</span></tt>)</li>
<li>Secondary troppo lenti a scrivere</li>
</ul>
</div></blockquote>
</li>
<li><p class="first"><strong>Updates per Second</strong>
Questo valore sta ad indicare le scritture al secondo effettuate dal sistema, se è alto
può avere effetto sulla <em>Replication Lag</em>.</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="comandi-monitoraggio-cluster">
<h2>Comandi Monitoraggio Cluster<a class="headerlink" href="#comandi-monitoraggio-cluster" title="Permalink to this headline">¶</a></h2>
<div class="section" id="analisi-workingset">
<h3>Analisi WorkingSet<a class="headerlink" href="#analisi-workingset" title="Permalink to this headline">¶</a></h3>
<p>Il <em>WorkingSet</em> è il carico di dati che MongoDB deve abitualmente manipolare in base
al carico di lavoro richiesto (inserimenti e query che vengono fatte). La RAM fornita
al sistema dovrebbe essere sempre superiore al working set, per evitare swap su disco e quindi
IO bloccante all&#8217;interno di MongoDB.</p>
<p>Per verificare il working set è possibile tramite il comando:</p>
<div class="highlight-python"><div class="highlight"><pre>&gt; db.runCommand( { serverStatus: 1, workingSet: 1 } )
</pre></div>
</div>
<p>L&#8217;output fornito conterrà una sezione:</p>
<div class="highlight-python"><div class="highlight"><pre>&quot;workingSet&quot; : {
        &quot;note&quot; : &quot;thisIsAnEstimate&quot;,
        &quot;pagesInMemory&quot; : 654978,
        &quot;computationTimeMicros&quot; : 115617,
        &quot;overSeconds&quot; : 723
},
</pre></div>
</div>
<p>In particolare il valore di interesse è <tt class="docutils literal"><span class="pre">pagesInMemory</span></tt> in cui è riportato il numero di
pagine di memoria usate. Di default ogni pagina di memoria ha una dimensione di <tt class="docutils literal"><span class="pre">4096</span> <span class="pre">bytes</span></tt>,
quindi l&#8217;occupzione del sistema in questo caso sarebbe di <tt class="docutils literal"><span class="pre">654978*4</span></tt> = <tt class="docutils literal"><span class="pre">2619912Kb</span></tt> ~ <tt class="docutils literal"><span class="pre">2.7Gb</span></tt>.</p>
<p>Un altro valore molto importate è <tt class="docutils literal"><span class="pre">overSeconds</span></tt>, questo indica il tempo passato tra la prima
e l&#8217;ultima pagina inserita in RAM. Se il valore è basso, vuole dire che il sistema sta
continuamente mettendo e togliendo pagine dalla RAM, quindi che la RAM non è sufficiente a contenere
il workingSet. Questo valore dovrebbe essere sempre di almeno qualche minuto.</p>
</div>
<div class="section" id="analisi-locking">
<h3>Analisi Locking<a class="headerlink" href="#analisi-locking" title="Permalink to this headline">¶</a></h3>
<p>Il <em>Locking</em> è il tempo che in MongoDB il sistema è fermo ad eseguire una operazione,
durante il locking nessun altra operazione può procedere, nemmeno la sincronzizazione
dei nodi nel replicaSet o il bilanciamento nello shard. Quindi è molto importante che sia
basso per permettere uno stato sano del cluster MongoDB.</p>
<p>Lo stato dei lock è sempre riportato dal comando
<tt class="docutils literal"><span class="pre">db.runCommand(</span> <span class="pre">{</span> <span class="pre">serverStatus:</span> <span class="pre">1,</span> <span class="pre">workingSet:</span> <span class="pre">1</span> <span class="pre">}</span> <span class="pre">)</span></tt> ed diviso in due sezioni.</p>
<p><tt class="docutils literal"><span class="pre">globalLock</span></tt> in cui è riportato lo stato generale del locking:</p>
<div class="highlight-python"><div class="highlight"><pre>&quot;globalLock&quot; : {
        &quot;totalTime&quot; : NumberLong(1435194000),
        &quot;lockTime&quot; : NumberLong(1319013),
        &quot;currentQueue&quot; : {
                &quot;total&quot; : 0,
                &quot;readers&quot; : 0,
                &quot;writers&quot; : 0
        },
        &quot;activeClients&quot; : {
                &quot;total&quot; : 0,
                &quot;readers&quot; : 0,
                &quot;writers&quot; : 0
        }
},
</pre></div>
</div>
<p>e <tt class="docutils literal"><span class="pre">locks</span></tt> in cui è riportato lo stato dei lock per ogni database:</p>
<div class="highlight-python"><div class="highlight"><pre>&quot;locks&quot; : {
    &quot;.&quot; : {
        &quot;timeLockedMicros&quot; : {
            &quot;R&quot; : NumberLong(0),
            &quot;W&quot; : NumberLong(1319013)
        },
        &quot;timeAcquiringMicros&quot; : {
            &quot;R&quot; : NumberLong(0),
            &quot;W&quot; : NumberLong(6564)
        }
    },
    &quot;admin&quot; : {
        &quot;timeLockedMicros&quot; : {
            &quot;r&quot; : NumberLong(19097),
            &quot;w&quot; : NumberLong(0)
        },
        &quot;timeAcquiringMicros&quot; : {
            &quot;r&quot; : NumberLong(4785),
            &quot;w&quot; : NumberLong(0)
        }
    },
    &quot;local&quot; : {
        &quot;timeLockedMicros&quot; : {
            &quot;r&quot; : NumberLong(72576),
            &quot;w&quot; : NumberLong(970)
        },
        &quot;timeAcquiringMicros&quot; : {
            &quot;r&quot; : NumberLong(12735),
            &quot;w&quot; : NumberLong(18)
        }
    },
    &quot;DB_elise&quot; : {
        &quot;timeLockedMicros&quot; : {
            &quot;r&quot; : NumberLong(34206),
            &quot;w&quot; : NumberLong(320)
        },
        &quot;timeAcquiringMicros&quot; : {
            &quot;r&quot; : NumberLong(10370),
            &quot;w&quot; : NumberLong(17)
        }
    },
}
</pre></div>
</div>
<p>Il valore <tt class="docutils literal"><span class="pre">r</span></tt> e <tt class="docutils literal"><span class="pre">R</span></tt> non sono problematici generalmente, essi sono lock condivisi e quindi
non bloccano in modo esclusivo il sistema. Mentre i lock di tipo <tt class="docutils literal"><span class="pre">w</span></tt> e <tt class="docutils literal"><span class="pre">W</span></tt> sono esclusivi
ed il loro tempo andrebbe minimizzato. In particolare confrontando i valori <tt class="docutils literal"><span class="pre">totalTime</span></tt> e
<tt class="docutils literal"><span class="pre">lockTime</span></tt> è possibile fare il rapporto di quanta % del tempo è stato da MongoDB bloccato.</p>
<p>In <tt class="docutils literal"><span class="pre">currentQueue</span></tt> è poi possibile verificare se ci sono operazioni in attesa di acquisire
il lock sul database.</p>
<p>Si può poi verificare quali operazioni stanno causando problemi di locking tramite il comando
<tt class="docutils literal"><span class="pre">db.currentOp()</span></tt> che va lanciato sul nodo che sta eseguendo l&#8217;operazione:</p>
<div class="highlight-python"><div class="highlight"><pre>&gt; db.currentOp()
{
    &quot;inprog&quot; : [
        {
            &quot;opid&quot; : 4882,
            &quot;active&quot; : true,
            &quot;secs_running&quot; : 1,
            &quot;op&quot; : &quot;getmore&quot;,
            &quot;ns&quot; : &quot;local.oplog.rs&quot;,
            &quot;query&quot; : {

            },
            &quot;client&quot; : &quot;127.0.0.1:54403&quot;,
            &quot;desc&quot; : &quot;conn134&quot;,
            &quot;threadId&quot; : &quot;0x7f082add0700&quot;,
            &quot;connectionId&quot; : 134,
            &quot;waitingForLock&quot; : false,
            &quot;numYields&quot; : 0,
            &quot;lockStats&quot; : {
                &quot;timeLockedMicros&quot; : {
                    &quot;r&quot; : NumberLong(35),
                    &quot;w&quot; : NumberLong(0)
                },
                &quot;timeAcquiringMicros&quot; : {
                    &quot;r&quot; : NumberLong(375),
                    &quot;w&quot; : NumberLong(0)
                }
            }
        },
        {
            &quot;opid&quot; : 4881,
            &quot;active&quot; : true,
            &quot;secs_running&quot; : 1,
            &quot;op&quot; : &quot;getmore&quot;,
            &quot;ns&quot; : &quot;local.oplog.rs&quot;,
            &quot;query&quot; : {

            },
            &quot;client&quot; : &quot;127.0.0.1:54402&quot;,
            &quot;desc&quot; : &quot;conn133&quot;,
            &quot;threadId&quot; : &quot;0x7f082aacd700&quot;,
            &quot;connectionId&quot; : 133,
            &quot;waitingForLock&quot; : false,
            &quot;numYields&quot; : 0,
            &quot;lockStats&quot; : {
                &quot;timeLockedMicros&quot; : {
                    &quot;r&quot; : NumberLong(335),
                    &quot;w&quot; : NumberLong(0)
                },
                &quot;timeAcquiringMicros&quot; : {
                    &quot;r&quot; : NumberLong(7),
                    &quot;w&quot; : NumberLong(0)
                }
            }
        }
    ]
}
</pre></div>
</div>
<p>Se una delle operazioni nell&#8217;elenco sta tenendo un lock per troppo lungo tempo ed altre
sono in <tt class="docutils literal"><span class="pre">waitingForLock</span></tt> a causa sua è necessario intervenire uccidendo l&#8217;operazione con
<tt class="docutils literal"><span class="pre">db.killOp(opid)</span></tt>, specialmente se le operazioni in <tt class="docutils literal"><span class="pre">waitingForLock</span></tt> hanno come
campo <tt class="docutils literal"><span class="pre">ns</span></tt> il valore <tt class="docutils literal"><span class="pre">local.oplog</span></tt> in quanto vuole dire che è bloccata la replicazione
dell&#8217;oplog e quindi il replicaSet rischia di non avere dati consistenti.</p>
</div>
<div class="section" id="analisi-replicaset">
<h3>Analisi ReplicaSet<a class="headerlink" href="#analisi-replicaset" title="Permalink to this headline">¶</a></h3>
<p>Lo stato del replica set può essere recuperato con il comando <tt class="docutils literal"><span class="pre">rs.status()</span></tt> eseguito
sui nodi del replica set stesso:</p>
<div class="highlight-python"><div class="highlight"><pre>&gt; rs.status()
{
    &quot;set&quot; : &quot;rs0&quot;,
    &quot;date&quot; : ISODate(&quot;2015-03-04T11:45:04Z&quot;),
    &quot;myState&quot; : 1,
    &quot;members&quot; : [
        {
            &quot;_id&quot; : 0,
            &quot;name&quot; : &quot;LPulsar:27018&quot;,
            &quot;health&quot; : 1,
            &quot;state&quot; : 2,
            &quot;stateStr&quot; : &quot;SECONDARY&quot;,
            &quot;uptime&quot; : 3631,
            &quot;optime&quot; : Timestamp(1425309401, 1),
            &quot;optimeDate&quot; : ISODate(&quot;2015-03-02T15:16:41Z&quot;),
            &quot;lastHeartbeat&quot; : ISODate(&quot;2015-03-04T11:45:03Z&quot;),
            &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-03-04T11:45:03Z&quot;),
            &quot;pingMs&quot; : 0,
            &quot;syncingTo&quot; : &quot;LPulsar:27020&quot;
        },
        {
            &quot;_id&quot; : 2,
            &quot;name&quot; : &quot;LPulsar:27020&quot;,
            &quot;health&quot; : 1,
            &quot;state&quot; : 1,
            &quot;stateStr&quot; : &quot;PRIMARY&quot;,
            &quot;uptime&quot; : 3633,
            &quot;optime&quot; : Timestamp(1425309401, 1),
            &quot;optimeDate&quot; : ISODate(&quot;2015-03-02T15:16:41Z&quot;),
            &quot;electionTime&quot; : Timestamp(1425465873, 1),
            &quot;electionDate&quot; : ISODate(&quot;2015-03-04T10:44:33Z&quot;),
            &quot;self&quot; : true
        },
        {
            &quot;_id&quot; : 3,
            &quot;name&quot; : &quot;LPulsar:27019&quot;,
            &quot;health&quot; : 1,
            &quot;state&quot; : 2,
            &quot;stateStr&quot; : &quot;SECONDARY&quot;,
            &quot;uptime&quot; : 3629,
            &quot;optime&quot; : Timestamp(1425309401, 1),
            &quot;optimeDate&quot; : ISODate(&quot;2015-03-02T15:16:41Z&quot;),
            &quot;lastHeartbeat&quot; : ISODate(&quot;2015-03-04T11:45:03Z&quot;),
            &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-03-04T11:45:04Z&quot;),
            &quot;pingMs&quot; : 0,
            &quot;syncingTo&quot; : &quot;LPulsar:27020&quot;
        }
    ],
    &quot;ok&quot; : 1
}
</pre></div>
</div>
<p>Nel comando è riportato quali nodi sono in stato <tt class="docutils literal"><span class="pre">SECONDARY</span></tt> e quale è il <tt class="docutils literal"><span class="pre">PRIMARY</span></tt>,
in particolare è indicato il campo <tt class="docutils literal"><span class="pre">optime</span></tt> in cui è indicato a quale punto dell&#8217;oplog
il nodo è arrivato. Questo dovrebbe coincidere tra tutti e tre i nodi affinché questi siano in
sincrono. Il campo <tt class="docutils literal"><span class="pre">optimeDate</span></tt> poi riporta a che data è aggiornato il nodo. Se un nodo è
troppo vecchio non potrà essere considerato contenere dei dati coerenti.</p>
<p>I campi <tt class="docutils literal"><span class="pre">lastHeartbeat</span></tt> e <tt class="docutils literal"><span class="pre">electionTime</span></tt> riportano invece rispettivamente l&#8217;ultima data
in cui i nodi sono riusciti a contattare gli altri nodi e la data in cui è stato eletto il
PRIMARY corrente.</p>
<p>Se <tt class="docutils literal"><span class="pre">optime</span></tt> figura troppo indietro può avere senso verificare in ordine:</p>
<blockquote>
<div><ul class="simple">
<li>Stato dei lock per essere certi che la replicazione non sia bloccata da altri processi</li>
<li>Performance della rete, per essere certi che i nodi riescano a trasferire quanto devono
(La rete deve poter sostenere un throughput pari a <em>numero inserimenti al secondo</em> per
<em>avgObjSize</em> riportato da <tt class="docutils literal"><span class="pre">db.stats()</span></tt>).</li>
<li>Performance del disco, queste possono essere verificate con il tool <tt class="docutils literal"><span class="pre">mongoperf</span></tt>.
(Ugualmente dovrebbero sostenere gli inserimenti al secondo per l&#8217;avgObjSize riportato
da <tt class="docutils literal"><span class="pre">db.stats()</span></tt>)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="analisi-bilanciatore-sharding">
<h3>Analisi Bilanciatore Sharding<a class="headerlink" href="#analisi-bilanciatore-sharding" title="Permalink to this headline">¶</a></h3>
<p>Lo stato dello sharding può esssere verificato con il comando <tt class="docutils literal"><span class="pre">sh.status()</span></tt>, esso
riporta per ognuno delle collezioni se sono in sharding (<tt class="docutils literal"><span class="pre">partitioned:</span> <span class="pre">true</span></tt>) e
quale è lo stato dei chunks:</p>
<div class="highlight-python"><div class="highlight"><pre>&gt; sh.status()
--- Sharding Status ---
  sharding version: {
    &quot;_id&quot; : 1,
    &quot;version&quot; : 4,
    &quot;minCompatibleVersion&quot; : 4,
    &quot;currentVersion&quot; : 5,
    &quot;clusterId&quot; : ObjectId(&quot;5491e70b39a5e85efe178d6e&quot;)
  }
  shards:
    {  &quot;_id&quot; : &quot;replshard1&quot;,  &quot;host&quot; : &quot;REPLICASET1_HOSTS&quot; }
    {  &quot;_id&quot; : &quot;replshard2&quot;,  &quot;host&quot; : &quot;REPLICASET2_HOSTS&quot; }
  databases:
    {  &quot;_id&quot; : &quot;DB_SUPPORT&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;replshard1&quot; }
    {  &quot;_id&quot; : &quot;DB_csp&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;replshard2&quot; }
DB_csp.measures
    shard key: { &quot;idDataset&quot; : 1, &quot;datasetVersion&quot; : 1, &quot;time&quot; : 1 }
    chunks:
        replshard1 3
        replshard2 3
        {
            &quot;idDataset&quot; : { &quot;$minKey&quot; : 1 },
            &quot;datasetVersion&quot; : { &quot;$minKey&quot; : 1 },
            &quot;time&quot; : { &quot;$minKey&quot; : 1 }
        } --&gt;&gt; {
            &quot;idDataset&quot; : 2,
            &quot;datasetVersion&quot; : 2,
            &quot;time&quot; : ISODate(&quot;2014-12-01T00:00:00Z&quot;)
        } on : replshard1 Timestamp(3, 0)
        {
            &quot;idDataset&quot; : 2,
            &quot;datasetVersion&quot; : 2,
            &quot;time&quot; : ISODate(&quot;2014-12-01T00:00:00Z&quot;)
        } --&gt;&gt; {
            &quot;idDataset&quot; : 2,
            &quot;datasetVersion&quot; : 2,
            &quot;time&quot; : ISODate(&quot;2014-12-02T12:52:00Z&quot;)
        } on : replshard2 Timestamp(2, 7)
        {
            &quot;idDataset&quot; : 43,
            &quot;datasetVersion&quot; : 1,
            &quot;time&quot; : ISODate(&quot;2014-12-10T17:00:00Z&quot;)
        } --&gt;&gt; {
            &quot;idDataset&quot; : { &quot;$maxKey&quot; : 1 },
            &quot;datasetVersion&quot; : { &quot;$maxKey&quot; : 1 },
            &quot;time&quot; : { &quot;$maxKey&quot; : 1 }
        } on : replshard1 Timestamp(2, 0)
</pre></div>
</div>
<p>I chunk sono i sottoinsiemi di dati che il bilanciatore ha deciso di dividere tra i vari
shard, sotto la voce <tt class="docutils literal"><span class="pre">chunks:</span></tt> sono indicati quanti chunk sono presenti su ognuno degli
shard. Questi valori dovrebbero essere quasi uguali tra gli shard, se così non fosse
vuole dire che il bilanciatore è spento (verificabile con <tt class="docutils literal"><span class="pre">sh.getBalancerState()</span></tt>) o che
sono presenti dei chunk <tt class="docutils literal"><span class="pre">jumbo</span></tt>.</p>
<p>I chunk jumbo sono insiemi di dati che a causa di una shard key non sufficientemente granulare
MongoDB non è stato in grado di dividere in sottoinsiemi. Questi sono identificabili dal fatto
che vicino alla descrizione del chunk riportata dal comando compare la parola <tt class="docutils literal"><span class="pre">jumbo</span></tt>.</p>
<p>Un&#8217;altra comune causa di una distribuzione inequa dei chunk è nel caso in cui la shard key
non riesca a garantire sufficiente cardinalità dei dati, se ad esempio la shard key è
sequenziale, tutti i dati verranno scritti assieme sullo stesso nodo ed il bilanciatore potrebbe
poi non riuscire a spostarli tra i nodi abbastanza in fretta, oltre a sovraccaricare il sistema
in quanto il dato deve essere:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Scritto sul nodo 1</li>
<li>Letto dal nodo 1</li>
<li>Scritto sul nodo 2</li>
<li>Cancellato dal nodo 1</li>
</ol>
</div></blockquote>
<p>Quindi per ogni scrittura vengono fatte poi altre 3 operazioni per ribilanciare i dati.</p>
<p>Un&#8217;altra possibile causa di problemi di bilanciamento è se viene impostata una <em>Balancing Window</em>,
(finestra di orari in cui il bilanciatore può girare per non sovraccarirare il sistamte)
essa potrebbe non essere sufficientemente grande da permettete al bilanciatore di spostare i dati.</p>
</div>
</div>
<div class="section" id="procedura-migrazione-config-server">
<h2>Procedura Migrazione Config Server<a class="headerlink" href="#procedura-migrazione-config-server" title="Permalink to this headline">¶</a></h2>
<p>Di seguito è descritta la procedura per migrare le instanze di configurazione
su nuove macchine. La procedura richiede il down del cluster qualora gli hostname
dei config server cambino:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Disabilitare il bilanciatore</li>
<li>Spegnere il config server da migrare</li>
<li>Copiare i dati del config server da migrare sul nuovo server (anche con scp va bene)</li>
<li>Avviare il nuovo config server</li>
<li>Spegnere tutti i nodi del cluster: mongos, mongod, config</li>
<li>Aggiornare l&#8217;opzione <tt class="docutils literal"><span class="pre">configdb</span></tt> nei mongos sostituendo il nuovo config</li>
<li>Riavviare tutto il cluster</li>
<li>Riaccendere il bilanciatore.</li>
</ol>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Lo spegnimento di un config server impedisce la migrazione dei chnuk sul database,
ma permette al sistema di continuare a funzionare anche in scrittura.</p>
</div>
</div>
<div class="section" id="mongodb-suggested-deploy">
<h2>MongoDB Suggested Deploy<a class="headerlink" href="#mongodb-suggested-deploy" title="Permalink to this headline">¶</a></h2>
<img alt="_images/mongodb-deploy.png" src="_images/mongodb-deploy.png" />
<p>Il deploy suggerito include <strong>3 Shard</strong> ognuno costituito da un Replica Set di <strong>5 nodi</strong>,
di cui <strong>4 operativi</strong> ed <strong>1 di backup</strong> (il nodo di backup è descritto in <a class="reference internal" href="backup.html#backup-replica"><em>Replica per Backup</em></a>).</p>
<p>Questo consente di scalare sul carico tramite l&#8217;uso dello shard e fornisce l&#8217;High Availability
fino ad un massimo di <em>2 nodi non disponibili</em> grazie all&#8217;operato del nodo di backup
<em>anche come arbitro del replica set</em>.</p>
<p>Il numero di shard minimo consigliato è 3 in quanto generalmente si ha un beneficio in termini
di performance superati i 2 shard. Finché il numero di shard è 2 o inferiore i benifici dello
sharding sono solitamente superati dal carico di lavoro extra dovuto allo shard stesso.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Note per la gestione del cluster MongoDB</a><ul>
<li><a class="reference internal" href="#oplog-consumption-estimation">OpLog Consumption Estimation</a></li>
<li><a class="reference internal" href="#scaling-orizzontale-vs-scaling-verticale">Scaling Orizzontale VS Scaling Verticale</a></li>
<li><a class="reference internal" href="#monitoraggio-cluster">Monitoraggio Cluster</a></li>
<li><a class="reference internal" href="#comandi-monitoraggio-cluster">Comandi Monitoraggio Cluster</a><ul>
<li><a class="reference internal" href="#analisi-workingset">Analisi WorkingSet</a></li>
<li><a class="reference internal" href="#analisi-locking">Analisi Locking</a></li>
<li><a class="reference internal" href="#analisi-replicaset">Analisi ReplicaSet</a></li>
<li><a class="reference internal" href="#analisi-bilanciatore-sharding">Analisi Bilanciatore Sharding</a></li>
</ul>
</li>
<li><a class="reference internal" href="#procedura-migrazione-config-server">Procedura Migrazione Config Server</a></li>
<li><a class="reference internal" href="#mongodb-suggested-deploy">MongoDB Suggested Deploy</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="availability.html"
                        title="previous chapter">Best Practices per Availability e Consistenza</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="upgrade_java_driver.html"
                        title="next chapter">Java driver assessment</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/sysadminnotes.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="upgrade_java_driver.html" title="Java driver assessment"
             >next</a> |</li>
        <li class="right" >
          <a href="availability.html" title="Best Practices per Availability e Consistenza"
             >previous</a> |</li>
        <li><a href="index.html">CSI MongoDB 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2015, AXANT.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>